Scoring Foundations
=====

```{r ,results="hide", echo=FALSE}
library(knitr)
opts_knit$set(base.dir = "scoring")
```

### The intuition behind

> Events can occur, or not... altough we don't have _tomorrow's newspaper_ :newspaper:, we can make a good guess about how is it going to be.

The future is undoublty attached to *uncertanty*, and this uncertanty can be estimated.

<br>

### And there are diferents targets...

By now this book will cover the classical: `Yes`/`No` target -also known as binary or multiclass prediction.

So, this estimation is the _value of truth_ of an event to happen, therefore a probabilist value between 0 and 1.

<br>

### Hey what? 

Some examples:
- Is this client going to buy this product?
- Is this patient going to get better?
- Is certain event going to happen in the next weeks? 

The answers to last questions are True or False, but **the essense is to have an score**, or a number indicating the likelihood of certain event to happen.

<br>

### But we need more control...

Many machine learning resources shows the simplified version -wich is good to start- getting the final class as an output. Let's say:

Simplified approach:

* Question: _Is this person going to have a heart disease?_ 
* Answer: "No"

But there is something else before the "Yes/No" answer, and this is the score:

* Question: _What is the likelihood for this person of having heart disease?_
* Answer: "25%"

<br>
So first you get the score, and then according to your needs you set the **cut point**. And this is **really** important.


### Let see an example
<img src='tbl_example_1.png' width='400px'> 

Example table showing the following
* `id`=identity
* `x1`,`x2` and `x3` input variables
* `target`=variable to predict


<img src='tbl_example_2.png' width='250px'> 

Forgetting about input variables... After the creation of the predictive model, like a random forest, we are interested in the **scores**. Even though our final goal is to deliver a `yes`/`no` predicted variable.


For example, following 2 sentences express the same: _The likelihood of being `yes` is `0.8`_ <=> _The likelihood of being `no` is `0.2`_

May be it is understood, but the score usually refers to the less representative class: `yes`.

--- 

**R Syntax** -_skip it if you don't want to see code_-

Following sentence will return the score:

`score = predict(randomForestModel, data, type = "prob")[, 2]`

Please note for other models this sintax may vary a little, but the concept **will remain the same**. Even for other languages.

Where `prob` indicates we want the probabilities (or scores). 

The `predict` function + `prob` parameter returns a matrix of 15 rows and 2 columns: the 1st indicates the likelihood of being `no` while the 2nd one indicates the same for class `yes`.

Since target variable can be `no` or `yes`, the `[, 2]` return the likelihood of being -in this case- `yes` (which is the complement of the `no` likelihood).

--- 

<br>

### It's all about the cut point

<img src='tbl_example_3.png' width='250px'> 

Now the table is ordered by score descendently.

It's to see how to extract the final class having by default the cut point in `0.5`. Tweeking the cut point will lead into a better classification.

> Accuracy metric or confusion matrix are always attached to a certain cut point value.

<br>

After assigning the cut point, we can see the clasification result getting the famous: 

* :white_check_mark:**True Positive** (TP): It's _true_, that the classification is _positive_, or, "the model hitted correctly the positive (`yes`) class".
* :white_check_mark:**True Negative** (TN): Same as before, but with negative class (`no`).
* :x:**False Positive** (FP): It's _false_, that the classification is _positive_, or, "the model missed, it predicted `yes` but the result was `no`
* :x:**False Negative** (FN): Same as before, but with negative class, "the model predicted negative, but it was positive", or, "the model predicted `no`, but the class was `yes`"


<img src='tbl_example_4.png' width='500px'> 

<br>

### The best and the worst escenario

> The analysis of extremes will help to find the right balance

:thumbsup: The best escenario is when **TP** and **TN** rates are 100%. That means the model correctly predicts all the `yes` and all the `no`; _(as a result, **FP** and **FN** rates are 0%)_.

But wait :raised_hand:! If you find a perfect classification, probably it's because of overfitting!

:thumbsdown: The worst escenario -the opposite to last example- is when **FP** and **FN** rates are 100%. Not even randomness can achieve such an awful escenario. 

_Why?_ If classes are balanced, 50/50, flipping a coin will assert around half of the results. This is common baseline to test if the model if better than randomness.

<br>
<br>

In the example provided, class distribution is 5 for `yes`, and 10 for `no`; so: 33,3% (5/15) is `yes`. 

<br>

---

### Comparing classifiers

#### Comparing classification results

:question: **Trivia**: Does a model which correcltly predict this 33.3% (TP rate=100%) a good one?

_Answer_: It depends on how many 'yes', the model said. 

<br>
A classifier that always predicts `yes`, will have a TP of 100%, but is absolutly useless since lots of `yes` will be actually `no`. As a matter of fact, FP rate will be high.


#### Comparing ordering label based on score 

A classifier must be trustful, and this is what **ROC** curves measures when plotting the TP vs FP rates. 

> The intuition behind ROC curve is to get an **score sanity measure** regarding how well it orders the label. Ideally all the positive labels must be at the top, and the negative at the bottom. 


<img src='tbl_example_5.png' width='500px'> 

<br>

`model 1` will have a higher Area Under Roc Curve (AUC) than `model 2`.

<img src='4_models_roc.png' width='500px'> 

Wikipedia has an extensive and good article on this: https://en.wikipedia.org/wiki/Receiver_operating_characteristic














