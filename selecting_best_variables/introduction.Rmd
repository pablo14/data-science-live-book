Selecting Variables Introduction
===

### What is this about?

This chapter will cover three types of plots which aim to understand what are the most correlated numeric variables against a target variable.


```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "selecting_best_variables")
```

* **Overview**:
    + **Analysis purpose**: To identify if the input variable is a good/bad predictor through visual analysis. 
    + **General purpose**: To explain the decision of including -or not- a variable to a model to a *non-analyst* person. 

<br>


### The "best" selection?

The chapter says "best", but we'd better mention a conceptual point: 

In general terms: _There is no unique best variable selection._ 

To start from this perspective is important, since in the exploration of many algorihms that _rank_ the variables according to their predictive power we can found different -and similar- results. That is

* Algorithm 1 has choosen as the best variable `var_1`, following by `var_5` and `var_14`.
* Algorithm 2 did this ranking: `var_1`, `var_5` and `var_3`.

Let's imagine based on algorithm 1, accuracy is 80%, while the accuracy based on algorithm 2 is 78%. Considering that every model has its inner variance, the result can be considered as the same. It reduces us time in pursuit of the perfect variable selection.

However going to the extremes, there will be a set of variables that will rank high across many algorithms, and the same goes for those with low predictive power. 


<br>

### Going deeper into variable ranking

It's quite common to find in literature and algorithms of selecting best variables an univariate analysis report of them, that is a ranking of variables given certain metric.

We're going to create two models: random forest and gradient boosting machine using `caret` R package to cross-validate the data. Next we'll compare the best variable ranking that every model return.

```{r}
library(caret)
library(funModeling)
library(dplyr)

## Excluding all NA rows from data, in this case NA are not the main issue to solve, we'll skip the 6 cases which have NA.
heart_disease=na.omit(heart_disease)

## setting cross-validation 4-fold
fitControl = trainControl(method = "cv",
                           number = 4,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)

## creating the random forest model finding the best tunning parameter set
set.seed(999)
fit_rf = train(x=select(heart_disease, -has_heart_disease, -heart_disease_severity),
             y = heart_disease$has_heart_disease,
             method = "rf",
             trControl = fitControl,
             verbose = FALSE,
             metric = "ROC")

## creating the random forest model finding the best tunning parameter set
fit_gbm = train(x=select(heart_disease, -has_heart_disease, -heart_disease_severity),
             y = heart_disease$has_heart_disease,
             method = "gbm",
             trControl = fitControl,
             verbose = FALSE,
             metric = "ROC")
```

Now we can proceed with the comparison. Columns: `importance_rf` and `importance_gbm` represent the importance measured by each algorithm. Based on each metric, there is the `rank_rf` and `rank_gbm` which represent the importance order, finally `rank_diff` represents how differnt each rank according comparing two algorithms.


```{r}
## next code is not important, it's creates the table described before...
var_imp_rf=data.frame(varImp(fit_rf, scale=T)["importance"]) %>% dplyr::mutate(variable=rownames(.)) %>% dplyr::rename(importance_rf=Overall) %>% dplyr::arrange(-importance_rf) %>% dplyr::mutate(rank_rf=seq(1:nrow(.))) 

var_imp_gbm=as.data.frame(varImp(fit_gbm, scale=T)["importance"])  %>% dplyr::mutate(variable=rownames(.)) %>% dplyr::rename(importance_gbm=Overall) %>% dplyr::arrange(-importance_gbm) %>% dplyr::mutate(rank_gbm=seq(1:nrow(.)))                                                                                                                                         
final_res=merge(var_imp_rf, var_imp_gbm, by="variable")

final_res$rank_diff=final_res$rank_rf-final_res$rank_gbm

# Printing the results!
final_res
```

We can see that there are variables which are not important at all to both models (`fasting_blood_sugar`). There are others that mantain a position at the top of importance like `chest_pain` and `thal`.

Different predictive models implementations have their own criteria to report what are the best features, according to that particular model. This ends up in different ranking across different algorithms. _More info about variable importance per algorithm can be found at <a href=https://topepo.github.io/caret/variable-importance.html" target="blank">caret documentation</a>._

Altough the ranking will vary from algorithm to algorithm, in general terms there is a correlation between all of these results as we mentioned before. But every ranking list is not the _"final truth"_, it gives us orientation about where the information is.

<br>

#### Advice in practice

* To pick up top _N_ variables from one (or more) ranking list and then build a model with only this selection.
* When possible, validate the list with someone who knows about the field study, the business and/or the area from the data comes from. Either to validate the top _N_ and the bottom _M_ variables. From those _bad_ variables we may be missing something in the data munging that could be destroying its predictive power whereas the expert domain says the relevance of such variable.

<br>

#### The nature of the selection

There are two ways of producing the variable selection:

* **Predictive model dependent**: Like the ones we saw before. The model will rank variables according to one instrinsic measure of accuracy. In tree based models metrics such as information gain, gini index, node impurity. Ref [4], [5].
* **Not Predictive model dependent**: This are quite interesting since they are no as popular as the other ones but they are prooved to perform really well in areas realated to genomic data. This area has the particularity that the amount of variables is really huge in comparisson with others. They need to find those _relevant_ genes (input variable) that are correlated with certain disease, like cancer (target variable).

<a href="https://academic.oup.com/bioinformatics/article-pdf/29/18/2365/659654/btt383.pdf" target="blank">Paper</a>

<br> 

### Improving variables

Variables can increase their predictive power by treating them. 

This book will covers -by now-:

* <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html" target="blank">improvement of categorical variables</a>.
* Reducing the noise in numerical variables through binning in <a href="http://livebook.datascienceheroes.com/selecting_best_variables/cross_plot.html"  target="blank">cross_plot</a> function.


### Cleaning by domain knowledge

This point is excluded from algorithmic procedures, it is related to the area in which the data was generated.

Imagine data coming from a survey. This survey has 1 year of history and during the first 3 months there was not a good control when inserting data, so users can type whatever they want. Variables during this period will probably be spourious. 

Easy to recognize when during an specific period data is empty or null. Also when there are extreme values. 

We should ask question: 

_Is it reliable this data?_ Keep in mind the predictive model will learn _as a kid_, it will not judge the data just learn from it. If data is spourious in an specific period, then remove these input cases.

To move on this point, we should get in touch a little with every input variable.

More info in : <a href="">data preparation</a> TODO: ADD LINK DATA PREP INTRO

<br>

### Variables work in groups

When selecting the _best_ variables, the main aim is to get those variables which carry the most information regarding a target, outcome or dependant variable. 

A predictive model will find its weights or parameters based on its 1 to 'N' input variables.

Variables usually don't work isolatelty when explaining an event. Quoting Aristoteles: 

> “The whole is greater than the sum of its parts.” 

This is also true when selecting the _best_ features: 

_Building a predictive model with two variables may reach a higher accuracy than the models built with only one variable._

For example: Building a model based on variable `var_1` could lead to an overall accuracy of 60%. On the other hand build a model based on `var_2` could reach an accuracy of 72%. But when we combine these two `var_1` and `var_2` variables, we could reach an accuracy above 80%. TODO: propose example.

<br>



### Correlation between variables

The ideal escenario is to build a predictive model with only non-correlated variables between them. In practice it's complicated to keep such a escenario for all variables. 

For sure there will be variables a set of variables that are not correlated between them, but also there will be others that share a little of correlation.

Quite important how to measure correlation. Results can be highly different based on linear or non-linear procedures. More info in the <a href="http://livebook.datascienceheroes.com/selecting_best_variables/correlation.html">correlation chapter</a>.

<br>

_What is the problem with adding correlated variables?_

The problem is we're adding complexity to the model: more time consuming, more difficult to understand-explain, less accurate, etc. This is an effect we reviewed in <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html#dont-predictive-models-handle-high-cardinality-part-2">reducing cardinality in categorical variables</a>. 


The general rule would be: Try to add top N variables that are correlated with the output, but not correlated between them. This lead us to the next section. 


<br>

### Keep it simple

The principle of **Occam's razor**: 

> Among competing hypotheses, the one with the fewest assumptions should be selected.

Re-interpreting this sentence for machine learning, those "hypotheses" can be seen as variables, so we've got: 

**Among different predictive models, the one with fewest variables should be selected.**

Of course, there is also the trade-off of adding-substracting variables and the accuracy of the model. 

A predictive model with a _high_ number of variables will tend to do **overfitting**. while on the other hand, a model with a _low_ number of variables will lead to do **underfitting**.

The concept of _high_ and _low_ is **highly subjective** to the data that is under analysis. In practice, we may have some accuracy metric, for example ROC value. In practice we would see something like:

<img src="variable_selection_table.png" alt="Quantity of variables vs ROC value trade-off"> 

Last picture shows different subset of variables and an accuracy metric (ROC). Each dot represents the ROC value given certain number of variables used to build the model.

We can check that the highest ROC is reached when the model is built with 30 variables. If we based the selection only in an automated process we may be choosing a subset which tend to overfit data. This report was produced by library `caret` in R [2], but is analogous to any software.

Take a closer look at the difference between the subset of 20 and the 30, there is only an improvement of **1.8%** -from 0.9324 to 0.95- choosing **10 more variables.** In other words: _Choosing 50% more variables will impact in less than 2% of improvement._

Even more, this 2% may be an error margin given the variance in prediction that every predictive model has [3].

**Conclusion:**

In this case, and being consequent with Occams Razor principle, the best solution is to build the model with the subset of 20 variables.

Explaining to others -and understanding- a model with 20 variables is easier than similar one with 30.

<br> 

### Summary



<br> 

**References:**

* [1] <a href="https://en.wikipedia.org/wiki/Occam's_razor#Probability_theory_and_statistics">Occam's razor in statistics</a>.
* [2] <a href="https://topepo.github.io/caret/recursive-feature-elimination.html">Recursive feature elimination in caret</a>
* [3] It is covered in the <a href="http://livebook.datascienceheroes.com/model_performance/knowing_the_error.html">Knowing the error</a> chapter.
* [4] Understanding <a href="http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain" target="blank">Entropy and Information Gain</a>.
* [5] Understanding the <a href="http://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore" target="blank">accuracy and gini index</a> used in random forest variable ranking.


