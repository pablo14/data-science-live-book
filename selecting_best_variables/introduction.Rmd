Selecting Variables Introduction
===

### What is this about?

This chapter will cover three types of plots which aim to understand what are the most correlated numeric variables against a target variable.


```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "selecting_best_variables")
```

* **Overview**:
    + **Analysis purpose**: To identify if the input variable is a good/bad predictor through visual analysis. 
    + **General purpose**: To explain the decision of including -or not- a variable to a model to a *non-analyst* person. 

<br>


### The "best" selection?

The chapter says "best", but we'd better mention a conceptual point: 

In general terms: _There is no unique best variable selection._ 

To start from this perspective is important, since in the exploration of many algorihms that _rank_ the variables according to their predictive power we can found different -and similar- results. That is

* Algorithm 1 has choosen as the best variable `var_1`, following by `var_5` and `var_14`.
* Algorithm 2 did this ranking: `var_1`, `var_5` and `var_3`.

Let's imagine based on algorithm 1, accuracy is 80%, while the accuracy based on algorithm 2 is 78%. Considering that every model has its inner variance, the result can be considered as the same.

However going to the extremes, there will be a set of variables that will rank high across many algorithms, and the same goes for those with low predictive power. 

<br>

### Ranking variables? There is not only one truth

It's quite common to find in literature and algorithms of selecting best variables an univariate analysis report of them, that is a ranking of variables given certain metric.

TODO: write example of heart_disease data variable ranking based on random forest, the two metrics, and gradient boosting machine.

Different predictive models implementations have their own criteria to report what are the best features, according to that particular model. This ends up in different ranking across different algorithms, such as the ones provided by random forest, gradient boosting machine, among others.

Altough the ranking will vary from algorithm to algorithm, in general terms there is a correlation between all of these results.

In practice as a quick solution: We can pick up top N variables from one of the ranking list, and then build a model with only those.

<br> 

### Improving variables

Variables can increase their predictive power by treating them. 

This book will covers -by now-:

* <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html" target="blank">improvement of categorical variables</a>.
* Reducing the noise in numerical variables through binning in <a href="http://livebook.datascienceheroes.com/selecting_best_variables/cross_plot.html"  target="blank">cross_plot</a> function.


### Cleaning by domain knowledge

This point is excluded from algorithmic procedures, it is related to the area in which the data was generated.

Imagine data coming from a survey. This survey has 1 year of history and during the first 3 months there was not a good control when inserting data, so users can type whatever they want. Variables during this period will probably be spourious. 

Easy to recognize when during an specific period data is empty or null. Also when there are extreme values. 

We should ask question: 

_Is it reliable this data?_ Keep in mind the predictive model will learn _as a kid_, it will not judge the data just learn from it. If data is spourious in an specific period, then remove this input cases.

To move on this point, we should get in touch a little with every input variable.

More info in : <a href="">data preparation</a> TODO: ADD LINK DATA PREP INTRO

<br>

### Variables work in groups

When selecting the _best_ variables, the main aim is to get those variables which carry the most information regarding a target, outcome or dependant variable. 

A predictive model will find its weights or parameters based on its 1 to 'N' input variables.

Variables usually don't work isolatelty when explaining an event. Quoting Aristoteles: 

> “The whole is greater than the sum of its parts.” 

This is also true when selecting the _best_ features: 

_Building a predictive model with two variables may reach a higher accuracy than the models built with only one variable._

For example: Building a model based on variable `var_1` could lead to an overall accuracy of 60%. On the other hand build a model based on `var_2` could reach an accuracy of 72%. But when we combine these two `var_1` and `var_2` variables, we could reach an accuracy above 80%. TODO: propose example.

<br>



### Correlation between variables

The ideal escenario is to build a predictive model with only non-correlated variables between them. In practice it's complicated to keep such a escenario for all variables. 

For sure there will be variables a set of variables that are not correlated between them, but also there will be others that share a little of correlation.

Quite important how to measure correlation. Results can be highly different based on linear or non-linear procedures. More info in the <a href="http://livebook.datascienceheroes.com/selecting_best_variables/correlation.html">correlation chapter</a>.

_What is the problem with adding correlated variables?_

The problem is we're adding complexity to the model: more time consuming, more difficult to understand-explain, less accurate, etc. This is an effect we reviewed in <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html#dont-predictive-models-handle-high-cardinality-part-2">reducing cardinality in categorical variables</a>. 


The general rule would be: Try to add top N variables that are correlated with the output, but not correlated between them. This lead us to the next section. 


<br>

### Keep it simple

The principle of **Occam's razor**: 

> Among competing hypotheses, the one with the fewest assumptions should be selected.

Re-interpreting this sentence for machine learning, those "hypotheses" can be seen as variables, so we've got: 

**Among different predictive models, the one with fewest variables should be selected.**

Of course, there is also the trade-off of adding-substracting variables and the accuracy of the model. 

A predictive model with a _high_ number of variables will tend to do **overfitting**. while on the other hand, a model with a _low_ number of variables will lead to do **underfitting**.

The concept of _high_ and _low_ is **highly subjective** to the data that is under analysis. In practice, we may have some accuracy metric, for example ROC value. In practice we would see something like:

<img src="variable_selection_table.png" alt="Quantity of variables vs ROC value trade-off"> 

Where we have different subset of variables and an accuracy metric (ROC). Each dot represents the ROC value given certain number of variables used to build the model.

We can check that the highest ROC is reached when the model is built with 30 variables. If we based the selection only in an automated process we may be choosing a subset which tend to overfit data. This report was produced by library `caret` in R [2], but is analogous to any software.

Take a closer look at the difference between the subset of 20 and the 30, there is only an improvement of **1.8%** -from 0.9324 to 0.95- choosing **10 more variables.** In other words: _Choosing 50% more variables will impact in less than 2% of improvement._

Even more, this 2% may be an error margin given the variance in prediction that every predictive model has [3].

**Conclusion:**

In this case, and being consequent with Occams Razor principle, the best solution is to build the model with the subset of 20 variables.

Explaining to others -and understanding- a model with 20 variables is easier than similar one with 30.

<br> 

### Summary



<br> 

**References:**

* [1] <a href="https://en.wikipedia.org/wiki/Occam's_razor#Probability_theory_and_statistics">Occam's razor in statistics</a>
* [2] <a href="https://topepo.github.io/caret/recursive-feature-elimination.html">Recursive feature elimination in caret</a>
* [3] It is covered in the <a href="http://livebook.datascienceheroes.com/model_performance/knowing_the_error.html">Knowing the error</a> chapter