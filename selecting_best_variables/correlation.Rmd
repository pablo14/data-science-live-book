Correlation and relationship
=====

### Overview


### Linear correlation (classical)

It retrieves the well known `R statistic` (or Pearson coefficient), which measures **linear** correlation  for all numeric variables _(skipping the string ones)_.


```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "selecting_best_variables")
```


```{r lib, results="hide"}
## Loading funModeling !
suppressMessages(library(funModeling))
data(heart_disease)
```

```{r}
correlation_table(data=heart_disease, str_target="has_heart_disease")
```

`R statistic` goes from `1` _positive correlation_ to `-1` _negative correlation_. A value around `0` implies no correlation.
Squaring this number returns the `R squared` statistic (aka `R2`), which goes from `0` _no correlation_ to `1` _high correlation_. 

#### R2 bias problem

**R statistic is highly influenced by outliers and non-linear relationships.**

Outliers can be treated with `prep_outliers` function, present in this package.

Take a look at the **Anscombe's quartet**. These 4 relationships are quite different, but all of them have the same R2: 0.816.


```{r, out.width = 400, fig.retina = NULL, echo=F}
library(knitr)
include_graphics("anscombe_quartet.png")
```



Last plot, and more info about correlation can be found at: [Correlation and dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence)

<br>

### Correlation based on Information Theory

This relationships can be measure better with <a href="https://en.wikipedia.org/wiki/Information_theory">Information Theory</a> conepts. One of the many algortihms to measure correlation based on this is: **MINE**, acronym for: Maximal Information-based nonparametric exploration.

The implementation in R can be found in <a href="https://cran.r-project.org/web/packages/minerva/index.html">minerva</a> package.


<br>

#### An example in R: Finding reliable variables

```{r, message=FALSE}
library(funModeling)
library(minerva)
library(ggplot2)
library(dplyr)
options(scipen=999)

# Important: If you recieve this message `Error: Missing values present in input variable 'x'. Consider using use = 'pairwise.complete.obs'.` is because data has missing values.

x=seq(0, 20, length.out=500)
df_exp=data.frame(x=x, y=dexp(x, rate=0.65))
ggplot(df_exp, aes(x=x, y=y)) + geom_line(color='steelblue') + theme_minimal()

# position [1,2] contains the correlation of both variables, excluding the correlation measure of each variable against itself.

# Calculating linear correlation
res_cor_R2=cor(df_exp)[1,2]^2
sprintf("R2: %s", round(res_cor_R2,2))

# now computing the MIC metric
res_mine=mine(df_exp)
sprintf("MIC: %s", res_mine$MIC[1,2])

```
**MIC** value goes from 0 to 1. Being 0 no correlation and 1 highest correlation. The intepretation is the same as the R squared.

<br>

### Analysis 

The `MIC=1` indicates there is a perfect correlation between the two variables. If we were doing **feature engineering** this variable should be included.

Further than a simple correlation, what the MIC says is: "Hey these two variables show a functional relationship". 

In machine learning terms (and oversimplifying): "variable `y` is dependant of variable `x` and a function -that we don't know which one- can be found model the relationship."

This is tricky, because that relationship was effectively created based on a function, an exponential one.

But let's continue with other examples...

<br>

### Adding noise

Noise is an undesire signal adding to the original one. In machine learning noise helps the model to get confuse. Concretly: two identical input cases -for example customers- have different outcomes -one buy and the other doesn't-.

Now we are going to add some noise creating the `y_noise_1` variable.

```{r}
df_exp$y_noise_1=jitter(df_exp$y, factor = 1000, amount = NULL)
ggplot(df_exp, aes(x=x, y=y_noise_1)) + geom_line(color='steelblue') + theme_minimal()
```

Calculating againg the correlation and MIC, printing in both cases the complete matrix, which shows the correlation/MIC metric of each input variable against all the others including themself.

```{r}
res_R2=cor(df_exp)^2

res_mine_2=mine(df_exp)

## R2 
res_R2

## MIC 
res_mine_2$MIC

```

Adding noise to the data decreases the MIC value from 1 to 0.7226365 (-27%), and this is great!

R2 also decreased but just a little bit, from 0.3899148 to 0.3866319 (-0.8%). 

**Conclusion:** MIC reflects a noisy relationship much better than R2. Due to the nature of noise, it is very common to find it on a regular basis.


<br>

### Measuring non-linearity (MICR2)

`mine` function returns several metrics, we checked only **MIC**, but due to the nature of the algorithm (you can check the original paper at ref. [1]), it computes more interesting indicators. Check them all by inspecting `res_mine_2` object.

One of them is `MICR2`, used as a measured of **non-linearity**. It is calculated by doing the: MIC - R2. Since R2 measures the linearity, a high `MICR2` would indicate a non-linear relationship.

We can check it by calculating the micr2 manually, following two matrix returns the same result:

```{r, eval=FALSE}
# MIC r2: non-linearity metric
round(res_mine_2$MICR2, 3)
# calculating MIC r2 manually
round(res_mine_2$MIC-res_R2, 3)
```

Non-linear relationships are harder to modelize, even more using a linear algorithm like decision trees or linear regression. 

Imagine we need to explain the relationship to other person, we'll need more details to explain it. It's easier to say: _"A increases as B increases and the ratio is always 3x"_ (if A=1 then B=3, linear); in comparisson to: _"A increases as B increases, but A is almost 0 until B reaches the value 10, then A raises to 300; and when B reaches 15, A goes to 1000._

```{r, message=FALSE}
x=df_exp$x
df_example=data.frame(x=df_exp$x, y_exp=df_exp$y, y_linear=3*df_exp$x+2)

res_mine_3=mine(df_example)

results_linear=sprintf("MIC: %s \n MICR2 (non-linearity): %s", res_mine_3$MIC[1,3],round(res_mine_3$MICR2[1,3],2))

results_exp=sprintf("MIC: %s \n MICR2 (non-linearity): %s", res_mine_3$MIC[1,2],round(res_mine_3$MICR2[1,2],4))

library(gridExtra)
p_exp=ggplot(df_example, aes(x=x, y=y_exp)) + geom_line(color='steelblue') + annotate("text", x = 11, y =0.4, label = results_exp) + theme_minimal()
p_linear=ggplot(df_example, aes(x=x, y=y_linear)) + geom_line(color='steelblue') + annotate("text", x = 8, y = 55, label = results_linear) + theme_minimal()
grid.arrange(p_exp,p_linear,ncol=2)
```

<br>

Both plots shows a perfect correlation (or relationship), holding a MIC=1.
Regarding non-linearity, MICR2 behaves as expected, in `y_exp`=0.6101, and in `y_linear`=0. 

This point is important since the **MIC behaves equals to R2 does in linear relationships**, plus it adapts quite well to **non-linear** relationships as we saw before, retrieving an specific score metric (`MICR2`) to profile the relationship. 

<br>

### Measuring non-monotonicity in time series (MAS)

MINE can also help us to profile time series regarding its non-monotonicity with **MAS** (maximun asimetry score).

A monotonic serie is such it never changes its tendency, it always goes up or down. More on this on ref [3].

Following example simualtes two time series, one not-monotonic `y_1` and the other monotonic `y_2`.

```{r}
# creating sample data (simulating time series)
time_x=sort(runif(n=1000, min=0, max=1))
y_1=4*(time_x-0.5)^2
y_2=4*(time_x-0.5)^3

# Calculating MAS for both series
mas_y1=round(mine(time_x,y_1)$MAS,2)
mas_y2=mine(time_x,y_2)$MAS

df_mono=data.frame(time_x=time_x, y_1=y_1, y_2=y_2)

p_y_1=ggplot(df_mono, aes(x=time_x, y=y_1)) + geom_line(color='steelblue') + theme_minimal()  + annotate("text", x = 0.45, y =0.75, label = sprintf("MAS=%s (goes down \n and up => not-monotonic)", mas_y1))

p_y_2=ggplot(df_mono, aes(x=time_x, y=y_2)) + geom_line(color='steelblue') + theme_minimal() + annotate("text", x = 0.43, y =0.35, label = sprintf("MAS=%s (goes up => monotonic)", mas_y2))

grid.arrange(p_y_1,p_y_2,ncol=2)

```


Summary:  

* MAS ~ 0 indicates monotonic function
* MAS ~ 1 indicates not-monotonic function

<br>

### Correlation on categorical (and binary) variables



### A really important issue

Further than correlation, MIC measures if there are a _functional relationship_. In this case the variable `y` is generated directly from a negative exponential function. In other words, a high MIC indicates that a function can generate the variable thus is likely to find a predictive model. 

<br>


### Just MINE covers this?

We are going to cover only MINE suite, but there are other algortihms related to <a href="http://www.scholarpedia.org/article/Mutual_information" target="blank">mutual information</a>. In R some of the packages are: <a href="https://cran.r-project.org/web/packages/entropy/entropy.pdf" target="blank">entropy</a> and <a href="https://artax.karlin.mff.cuni.cz/r-help/library/infotheo/html/mutinformation.html" target="blank">infotheo</a>.

help(mine) and run examples.

<br>

### References

[1] Original MINE paper: <a href="http://science.sciencemag.org/content/334/6062/1518.full?ijkey=cRCIlh2G7AjiA&keytype=ref&siteid=sci" target="blank">Detecting Novel Associations in Large Data Sets</a>.

[2] Some uses and explanations of MINE measurments in clinical data <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3561932/" target="blank">Characterizing Non-Linear Dependencies in clinical data</a>

[3] Wikipedia <a href="https://en.wikipedia.org/wiki/Monotonic_function" target="blank">Monotonic function</a>