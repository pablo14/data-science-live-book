Data Preparation Introduction
===

### What is this about?

In practice, 90% of the time is spent in data preparation.

```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "data_preparation")
```


### Removing outliers 

**Model building**: Some models such as random forest and gradient boosting machines tend to deal better with outliers, but some noise will affect results anyway. 

**Communicating results:** If we need to report the variables used in the model, we'll end up removing outliers to not see an histogram with only one bar, and/or show not a biased mean. 

It's better to show a non-biased number than justifying the model _will handle_ extreme values.

**Type of outliers:** 

* Numerical: For example the ones which bias the mean.
* Categorical: Having a variable in which the dispersion of categories is quite high (high cardinallity). For example: postal code.

Jump to <a href="http://livebook.datascienceheroes.com/data_preparation/outliers_treatment.html">Outliers Treatment</a> chapter.

<br>

### Don't use information from the future

<img src="back_to_the_future.png" width='250px'> 

Common mistake when starting a new predictive model project, for example:

Imagine we need to build a predictive model to know what users are likely to adquire full subscription in a web application, and this software has a ficticious feature called it `feature_A`:


```{r ,echo=FALSE}
d1=data.frame(user_id=rep(1:10), 
              feature_A=c("yes","yes","yes","no","yes","no","no","no","no","no"),
              full_subscription=c("yes","yes","yes","no","yes","no","no","no","no","no")) 
knitr::kable(d1, digits = 2)
```


We build the predictive model, we got a perfect accuracy, and an inspection throws the following: _"100% of users that have full subscription, uses Feature A"_. Some predictive algorithms report variable importance, thus `feature_A` will be at the top.

**The problem is:** `feature_A` is only availble **after the user goes for full subcription**. Therefore it cannot be used.

**The key message is**: Don't trust in perfect variables, nor perfect models. 

<br>

### Play fair with data, let it to develop their behavior

Like in nature, things have a minimun and maximun time to start showing certain behavior. This time oscilates from 0 to infinite. In practice it's recommended to study what is the best period to analyze, in other words, we may exclude all the behavior before and after this observation period. To stablish cuts to data it's not so straight forward since it may be kind of subjective.

Imagine we've got a **numerical variable** which increases as time moves. We may need to define an **observation time window** to filter the data and feed the predictive model. 

* Setting the **minimun** time: How much time is it need to start seeing the behavior? 
* Setting the **maximun** time: How much time is it needed to see the end of the behavior? 

Eassiest solution is: setting minimun since begin, and the maximun as the whole history.


<font color="#5d6d7e">

**Case study:** 

Two people, `Ouro` and `Borus`, are users of a web application which has certain functionalty called `feature_A`, and we need to build a predictive model which forecast based on `feature_A` usage  -measured in 'clicks", if the person is going to adcquire `full_subscription`.

The current data says: Borus has `full_subscription`, while Ouro doesn't. 

```{r echo=FALSE, fig.height=2, fig.width=3}
library(ggplot2)
d4=data.frame(user=c('Ouro','Ouro','Ouro','Ouro','Ouro','Borus','Borus','Borus','Borus','Borus', 'Ouro','Ouro'), days_since_signup=c(1,2,3,4,5,1,2,3,4,5,6,7), feature_A=c(2,3,3,5,12, 0, 0, 1, 6, 15, 20,24), stringsAsFactors = F)
pd4=ggplot(d4, aes(x=days_since_signup, y=feature_A, colour=user)) + geom_line() +  scale_x_continuous(breaks = 1:7) + theme(panel.grid.minor = element_blank())
plot(pd4)
```


User `Borus` starts using `feature_A` from day 3, and after 5 days she has more use -15 vs 12 clicks- on this feature than `Ouro` who started using it from day 0. 

If `Borus` adcquiere `full subscription` and `Ouro` doesn't, _what would the model will learn?_ 

When modeling with full history -`days_since_signup = all`-, the higher the `days_since_signup` the higher the likelihood, since `Borus` has the highest number.

However, if we keep only with the user history corresponding to their first 5 days since signup, the conclussion is the opposite. 

_Why to keep first 5 days of history?_

The behavior in this kick-off period may be more relevant -in terms of prediction accuracy- than analyzing the whole history. It depends on each case as we said before. 

<br> 

### Creating new variables 

It's a really important matter, to create new variables that reflect different stories from data. 

Other ideas to handle time variables:

* 
* Create two variables: quantity of clicks on first 5 days since signup, and quantity of clicks on the period from 6 to 30 days.
* Last point is considering the begining, but we can capture the history up to now: quantity of clicks during last month, during last 3 months, etc.
* Average variables per time: `monthly_clicks_average`.


</font>


------
