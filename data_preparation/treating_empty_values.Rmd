Handling Missing Data
===

### What is this about?

It was a case in which effectively an empty represents a `0`, nevertheless, other times the empty is just an empty.

Empty values are also known as `NULL` in data bases, `NA` in R or just the empty string in Spreadsheet. It also can be represented by some number like: "`-1`", "`-999`". 

And we will cover the analysis of empty values, as well as how to treat with them by transforming and inputing the variables.


<br>

### When the empty value represents information

For example, imagine a travel agency which joins two tables, one of persons and another of countries. The result shows the number of travels per person: 

```{r, echo=FALSE}
df_travel=data.frame(person=c("Fotero", "Herno", "Mamarul"), South_Africa=c(1, NA, 34), Brazil=c(5,1,40), Costa_Rica=c(5,NA,NA), stringsAsFactors = F)
df_travel
```

E.g. person `Mamarul` traveled `34` times to `South_Africa` country.

_What does `NA` (or NULL) value represents there?_

In this case `NA` should be replaced by `0`. Indicating zero travels in that person-country intersection. After the conversion, the table is ready to be used:

**Example in R:**

```{r}
# Reeplacing all 0 to NA values
df_travel[is.na(df_travel)]=0
df_travel
```

Last example transforms **all** `NA` values into `0`. However in other scenarios, some columns of the date may be apply to this transformation, and others don't.

Let's go to a more complex example.

<br>

### When the empty value, is an empty value

Another times the empty value is correct. We need to treat them in order to use the table. Many predictive models doesn't handle input tables with missing value.

Perhaps they start measuring certain variable _after_ a period of time, so we have data after this point and `NA` before. 

Other times there are random cases, like a machine which fails in collecting the data, an user who forgot to complete some field in a form, among others.

One important question arises: _What to do?!_ðŸ˜±

Following recommenations are just that, recommendations. You can try different approaches discovering the best strategy to the data your are analyzing.

<br>

### Excluding the entire row

If at least one column has an NA value, then exclude the row.

A fast and easy method, right? It's recommended when the amount of rows are really _low_. But how low is low? That's up to you. 10 cases in 1000 of rows _may not_ have a huge impact. Unless those 10 cases are related with an anomaly event to predict, in this case it represents information. We pointed out this issue in <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html#case-1-reducing-by-re-categorizing-less-representative-values" target="blank">case 1: reducing by re-categorizing less representative values.</a>

<br>

**Example in R:**

Let's inpsect `heart_disease` data, with `df_status` function, which one of its main objectives is to help us in this kind of decisions. 

```{r, message=FALSE}
library(dplyr)
library(funModeling)
df_status(heart_disease, print_results = F) %>% select(variable, q_na, p_na) %>% arrange(-q_na)
```

`q_na` indicates quantity of `NA` values, and `p_na` is the percentage. Full info about `df_status` can be found in <a href="http://livebook.datascienceheroes.com/data_preparation/profiling.html" target="blank">Profiling chapter</a>

Two variables have 4 and 2 rows with `NA` values, let's exclude these rows:

```{r}
# na.omit returns the same data frame having excluded all rows containing at least 1 NA value
heart_disease_clean=na.omit(heart_disease)
nrow(heart_disease) # number of rows before exclusion
nrow(heart_disease_clean) # number of rows after exclusion
```

After the exclusion, 6 rows out of 303 were eliminated. This approach seems suitable for this data set.

But, there are other scenarios in which there are almost no cases that do not contain empty values. The exclusion will delete the entiere data!

<br>

### Excluding the column

Similar to last case, but excluding the column. If we applying the same reasoning, if the deletion is about a _few_ columns, and the remaining provide a good final result, it may be ok. 

**Example in R:**

This exclusions are easily handle with `df_status` function. Next code will keep all variable names which percentage of `NA` values are higher than `0`.

```{r}
## Getting variables names with NA values
vars_to_exclude=df_status(heart_disease, print_results = F) %>% filter(p_na > 0) %>% .$variable

## Checking variables to exclude
vars_to_exclude

## Excluding variables from original data
heart_disease_clean_2=select(heart_disease, -one_of(vars_to_exclude))
```




<br>


### Treating empty in categorical variables

We cover different perspectives to convert as well as treat empty values in nominal variables.

The data of next example is `web_navigation_data` which contains typical information regarding how users come to certain web page. It contais `source_page` (the page from the visitor comes from), `landing_page` (first page to visit in our site) and `country`.

```{r}
# Reading example data, pay attention to na.strings parameter.
web_navigation_data=read.delim(file="web_navigation_data.txt", sep="\t", header = T, stringsAsFactors=F, na.strings="")
```

**Profiling the data:**

```{r}
stat_nav_data=df_status(web_navigation_data)
```

The three variables have empty (`NA`) values. Almost half of values in source_page are missing, while the other two variables have 5 and 3% of `NA`.

#### Case A: Convert the empty value into string

In categorical or nominal variables, the quickest treatment is to convert the empty value into the string `"unknown"`. So the machine learning model will handle the "empty" values as another category. Think about it in terms of  rules: `If variable_X="unknown" then outcome="yes"`.

Next we propose two methods intended to cover common escenarios.

**Example in R:**

```{r}
## Method 1: Converting just one variable and create a new variable:
web_navigation_data_1=web_navigation_data # making a copy

# creating a new variable
web_navigation_data_1$source_page_2=ifelse(is.na(web_navigation_data$source_page), "unknown_source", web_navigation_data$source_page)

## Method 2: It's a common situation only apply a function to certain variables and return the original data frame
# First we define the conversion function
convert_categ<-function(x) 
{
  # If 'NA' then put 'unknown', othwewise return the same value
  ifelse(is.na(x), "unknown", x)                                                                 
}                                 

# We obtain the variables to process, imagine we want to convert all variables with less than 6% of NA values:
vars_to_process=filter(stat_nav_data, p_na<6) 

# create the new data frame, with the transformed variables
#  Adding a minus in front of one_of(vars_to_process$variable) will apply the function to all columns except to the define ones
web_navigation_data_2=web_navigation_data %>% mutate_each(funs(convert_categ), one_of(vars_to_process$variable))
  
```
Checking the results:

```{r}
df_status(web_navigation_data_2)
```

<br>

#### Case B: Assign the most frequent category

The intuition behind it is _add more of the same so it does not affect_. But sometimes it does. It will not have the same impact if the most frequeny value appears 90% of the times, that if it does 10%. It depends on the distribution.

This technique is more suitable in a predictive model that is running on production, and a new value in a categorical variables. If the predictive model is robus, like **random forest** does, it will throw the message: `New factor levels not present in the training data.`, where `factor level` equals to "new category value". 

This book covered this point in <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html"
 target="blank">High Cardinality Variable in Predictive Modeling</a>.

As you can see the situation is not the same if we are building a predictive model to go live or doing an ad-hoc report.


<br>

#### Case C: Exclude some columns and transform others

The easy case is if the column contains, let's say, 50% of NA cases, making it highly likely to not be reliable. 

In the case we saw before, `source_page` have more than half of the values empty. We could exclude this variable and transform -as we did- the remaining two.

The example is prepared to be generic:

```{r}
# Setting the threshold
threshold_to_exclude=50 # 50 reprents 50%
vars_to_exclude=filter(stat_nav_data, p_na>=threshold_to_exclude) 
vars_to_keep=filter(stat_nav_data, p_na<threshold_to_exclude) 

# Finally...
vars_to_exclude$variable
vars_to_keep$variable

# Next line will exclude variables above the threshold, and it will transform the remaining ones
web_navigation_data_3=select(web_navigation_data, -one_of(vars_to_exclude$variable)) %>% mutate_each(funs(convert_categ), one_of(vars_to_keep$variable))

# Checking, there is no NA values and the variable above NA threshold disppaeared
df_status(web_navigation_data_3)

```

<br>  

#### Summing-up 

But how about if it contains 40% of NA values? It's depend on what the objective is, and the data. 

The important point here is "to save" the variable so we can use it. It's common to find many variables with missing values. May be those _incomplete variables_ carries good predictive information when they have a value, so we need to treat them and then build the predictive model. 

But, we need to minimize the bias we are introducing, because the missing value is a value that "is not there".

* When doing a report the suggest is to reeplace with the string "empty".
* When doing a predictive model that ir running live is to assign the most repetitive category. 

<br>

### Is there any pattern in missing values?

First load the example movie data. And do a quick a  quick profile.

```{r}
# Lock5Data contains many data frames to practice
library(Lock5Data)

# loading data
data("HollywoodMovies2011")

# profiling
df_status(HollywoodMovies2011)
```

Let's take a look at the values present in `p_na` column, there is a pattern in the missing values, 4 variables have `1.47`% of NA values, and another 4 have around `11.7`%. In this case are not able to check the data source, but it is a good idea to check if those cases have common issue.

<br> 

### Treating missing values in numerical variables

We've already approached this point in the begining of the chapter by converting all `NA` values into `0`.

One solution is to reeplace the empty by the mean, median or other criteria. But we have to be aware about the change in the distribution.

If the see that the variable seems to be coorelated when it's not empty (same as categorical), another method is to create bins, also known as buckets or segments, converting it into categorical.


#### Method 1: Converting into categorical

The function `equal_freq` splits the variable into the desiere bins:

```{r, fig.height=3, fig.width=6, echo=F, out.width = "400px"}
HollywoodMovies2011$TheatersOpenWeek_2=equal_freq(HollywoodMovies2011$TheatersOpenWeek, n_bins=5)

freq(HollywoodMovies2011, "TheatersOpenWeek_2")
```

As we can see, `TheatersOpenWeek_2` contains 5 buckets of 24 cases each where each one represents 20% of total cases. But the NA values are still there.

Finally we have to convert the `NA` into the string `empty`.

```{r, fig.height=3, fig.width=6, echo=F, out.width = "400px", results=FALSE}
HollywoodMovies2011$TheatersOpenWeek_2=as.character(HollywoodMovies2011$TheatersOpenWeek_2)
HollywoodMovies2011$TheatersOpenWeek_2=ifelse(is.na(HollywoodMovies2011$TheatersOpenWeek_2), "empty",  HollywoodMovies2011$TheatersOpenWeek_2)

freq(HollywoodMovies2011, "TheatersOpenWeek_2")
```

And that's it, the variable is ready to be used. 

**Custom cuts**:
If we want to use custom bucket sizes, not the ones provided by equal frequency, we can do the following:

```{r}
options(scipen=999) # disabling scientific notation in current R session

# Creating custom buckets, with limits in 1000, 2300 and a max of 4100. Values above 4100 will be assign as NA.

HollywoodMovies2011$TheatersOpenWeek_3=cut(HollywoodMovies2011$TheatersOpenWeek, breaks = c(0, 1000, 2300, 4100), include.lowest = T, dig.lab = 10)

freq(HollywoodMovies2011, "TheatersOpenWeek_3", plot = F)
```


It should be noted that **equal frecuency binning**, it tend to be more roubst than the equal distance which splits the variable taking the min and max and separating regardless how many cases fall in each bin. 

The equal frequency puts the outliers values in the first or last bin as appropriate. Normal values can range from 3 to 20 buckets. Higher number of buckets tend to be more noisy. More info about check the <a href="http://livebook.datascienceheroes.com/selecting_best_variables/cross_plot.html" target="blank">`cross_plot`</a> chapter function.

<br>

#### Method 2: Filling the NA with some value

Same as categorical, we can reeplace by a number such as the mean, the meadian. 

In this case we'll reeplace by the mean of the variable, and plotting the before and after side by

```{r, fig.height=3, fig.width=6, warning=FALSE, message=FALSE}
# Filling all NA values with the mean of the variable
HollywoodMovies2011$TheatersOpenWeek_mean=ifelse(is.na(HollywoodMovies2011$TheatersOpenWeek), mean(HollywoodMovies2011$TheatersOpenWeek, na.rm = T), HollywoodMovies2011$TheatersOpenWeek)

# Plotting original variable
p1=ggplot(HollywoodMovies2011, aes(x=TheatersOpenWeek)) + geom_histogram(colour="black", fill="white") + ylim(0, 30)
 
# Plotting transformed variable
p2=ggplot(HollywoodMovies2011, aes(x=TheatersOpenWeek_mean)) + geom_histogram(colour="black", fill="white") + ylim(0, 30)

# Putting the plots side by side 
library(gridExtra)
grid.arrange(p1, p2, ncol=2)
  
```

We can see a peak around the `2828` product of the transformation. This introduces a bias around this point. If we are predicting some event, it would be safer to not have some special event around this value. 

For example, if we are predicting an binary event, and the least representative event is correlated with having a mean of `3000` in `TheatersOpenWeek`, then the odds of having a higher **False Positive rate** may be higher.  Again the link to the <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html"
 target="blank">High Cardinality Variable in Predictive Modeling</a> chapter.

An extra comment regarding last visualization, it was important to set the y-axis maximum to 30 in order to make the plots comparable.

As you can see, there is an inter-relationship between all concepts.

<br>
  
#### Picking up the right value to fill 

Last example reeplaced the NA with the mean, but how about other value? It depends on the distribution of the variable.

The variable we used, `TheatersOpenWeek` _seems normally distributed_, that's why we used the `mean`. But if the variable is more skewed, probably another metric is more suitable. For example the `median`, which is less sensitive to outliers. 

<br>

### Advanced imputation methods

Now we are going to do a quick review about more complex imputation methods, in which we create a predictive model, with all it implies.

<br>

#### Method 1: Using MICE approach

MICE stands for "Multivariate Imputation by Chained Equations", also known as: Fully Conditional Specification. This book covers a minimal aspect of it since its popularity.

It is a whole framework to analyze and deal with missing values. It considers the iteraction among **all variables** at the same time, (multivariate) not just one, and bases its functionality on an **iterative** process which uses different predictive models to fill each variable. 

Internally it fill variable `A`, based on `B` and `C`. Then it fill `B` based on `A` and `C` (`A` previously predicted). And the iteration continues... The name "chained equations" comes from the fact that we can specfied the algorithm per variable to impute the cases.

It creates `M` replications of the original data with no missing values. _But why create `M` replications?_ 

In each replication the decision of what value insert in the _empty slot_ is based on a distribution. 

Many `mice` demostrations focus on validating the imputation and using the predictive models that supports the package, which are only a few. This is great if we don't want to use other predictive model (random forest, gradient boosting machine, etc), nor a cross-validation technique (`caret` for example). 

`mice` puts the final result by calling a `pool()` function which averages the parameters (or betas) of the `M` predictive models providing facilities to measure the variance due to missing values. 

Yes, one model per each generated data frame. Sounds like <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" target="blank">bagging</a>, isn't it? But we don't have this possibility with the mentioned models. 

`mice` has many functions to help us the process and validate the filling results. But to keep it very simple, we'll cover just a little part of it. Following example will focus on extracting a **data frame with no-missing values ready to be used** with other program or predictive model. 

**Example in R:**

It will impute data for `nhanes` data frame coming in `mice` library. Let's check it:

```{r, message=FALSE}
# install.packages("mice")
library(mice)
df_status(nhanes)
```

Three variables have missing values. Let's fill it:

```{r}
# Default imputation, it creates 5 complete data sets
imp_data=mice(nhanes, m = 5, printFlag = FALSE)

# Get a final data set containing the 5 imputed data frames, total rows=nrow(nhanes)*5
data_all=complete(imp_data, "long")

# data_all contains the same columns as nhanes plus 2 more: '.id' and '.imp'
# .id=row number, from 1 to 25
# .imp=imputation data frame id, 1 to 5 ('m' parameter)
```

Original data, `nhanes`, has 25 rows, `data_all` contains 125 rows, which is the result of creating 5 (`m=5`) complete data frames of 25 rows each. 

Time to check the results:

```{r, fig.height=3, fig.width=6}
densityplot(imp_data)
```

Each red line shows  the distribution of each imputed data frame, and the blue one contains the original distribution. The idea behing it is, if they look similar, then the imputation followed the original disitrbution. 

For example, `chl` contains one imputed data frame -thus only one red line- containing two peaks around two values much higher than the original one.  

The drawbacks are it is slow process, and it may requiere some tunning to work. For example: `mice_hollywood=mice(HollywoodMovies2011, m=5)` will fail after some time processing it, and it is an small data frame.

<a href="https://datascienceplus.com/handling-missing-data-with-mice-package-a-simple-approach/" target="blank"></a>
 
Original `mice` paper: <a href="https://www.jstatsoft.org/article/view/v045i03" target="blank">Multivariate Imputation by Chained Equations in R.</a>

<br>

#### Method 2: Using random forest (missForest)

`missForest` package 

```{r, message=FALSE}
library(missForest)

# copying the data
df_holly=HollywoodMovies2011

# We will introduce more 15% more of NA values in TheatersOpenWeek_3 to produce a better example. Function 'prodNA' in missForest will help us.
set.seed(31415) # to get always the same number of NA values...
df_holly$TheatersOpenWeek_3=prodNA(select(df_holly, TheatersOpenWeek_3), 0.15)[,1]

# missForest failed if it has any character variable, so we convert the only character into factor:
df_holly$TheatersOpenWeek_2=as.factor(df_holly$TheatersOpenWeek_2)

# excluding the Id column
df_holly=select(df_holly, -Movie)

# Now the magic! Imputing the data frame. xmis parameter=the data with missing values
imputation_res=missForest(xmis = df_holly)

# Final imputed data frame
df_imputed=imputation_res$ximp
```

Now it's time to compare the distribution of some of the imputed variables. Hopefully, they will look similar at a visual analysis. 


```{r, warning=FALSE, message=FALSE}
# Creating another imputation based on na.rougfix from random forest pacakge
df_rough=na.roughfix(df_holly)

# Compare distributions before-after imputation
df_holly$imputation="original"
df_rough$imputation="na.roughfix"
df_imputed$imputation="missForest"

# Putting the two data frames in only one, but split by is_imputed variable
df_all=rbind(df_holly, df_imputed, df_rough)

# converting to factor for using in a plot
df_all$imputation=factor(df_all$imputation, levels=unique(df_all$imputation))

# Plotting
ggplot(df_all, aes(TheatersOpenWeek, colour=imputation)) + geom_density() + theme_minimal() + scale_colour_brewer(palette="Set2")
```


* The green curve shows the distribution after the imputation based on `missForest` package.
* The orange shows the imputation method we talked at the beginning, it reeplaces all `NA` by the median, it is done by `na.roughfix` coming in `randomForest` package.
* The blue one shows the distribution without any imputation (of course, `NA` values are not displayed). 

**Analysis:**

Reeplace by the median tends to concentrate -as we expected- all the values around 3000. While the imputation given by `missForest` package does a **more natural distribution** since it doesn't concentrate around a single value. That's why the peak around 3000 is lower than the original one.

The orange and blue look pretty similar!

If we want to analyze from the analytical point of view, and don't play to be an art critic looking at a painting, we may run an statistical test comparing, for example, the means or the variance.
 
<br>

```{r}
# an ugly hack to plot NA as a category...
levels(df_all$TheatersOpenWeek_3)=c(levels(df_all$TheatersOpenWeek_3), "NA")
df_all$TheatersOpenWeek_3[is.na(df_all$TheatersOpenWeek_3)]="NA"

# now the plot!
ggplot(df_all, aes(x = TheatersOpenWeek_3, fill = TheatersOpenWeek_3)) +
    geom_bar(na.rm=T) + facet_wrap(~imputation)  + geom_text(stat='count',aes(label=..count..),vjust=-1) + ylim(0, 125) + scale_fill_brewer(palette="Set2") + theme_minimal() + theme(axis.text.x=element_text(angle = 45, hjust = 0.7))


```

<br>


**Analysis:**

Original variable contains 31 `NA` values which were reeplaced, by using the mode (most frequent value) in `na.roughfix`, and with an smoother and more reasonable cirteria using `missForest`.

`missForest` added 2 rows in category `"[0, 1000]"`, 1 in `"(1000,2300]"` and 32 in the `(2300,4100]`. While `na.roughfix` added 35 only to `(2300,4100]`.


<br>

### Conclusions

The Imputation methods when the missing values, missing value, a **controlled-bias**, to we have not to exclude the variables from the analysis nor the predictive model.

<br>