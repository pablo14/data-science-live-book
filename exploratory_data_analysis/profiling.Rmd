```{r ,results="hide", echo=FALSE}
library(knitr)
knitr::opts_chunk$set(out.width="400px", dpi=120)
knitr::opts_knit$set(base.dir = "exploratory_data_analysis")
```

# Profiling Data


## What is this about?

Quantity of zeros, NA, Inf, unique values; as well as the data type may lead to a good or bad model. Here's an approach to cover the very first step in data modeling. 


```{r lib, results="hide", message=FALSE}
## Loading funModeling !
library(funModeling)
library(dplyr)
data(heart_disease)
```

## Checking missing values, zeros, data type and unique values

Probably one of the first steps when we got a new data set to analyze, is to know if there are missing values (`NA` in **R**), and  the data type.

The `df_status` function coming in `funModeling` can help us showing these numbers in relative and percentual values.  Also it retrieves the infinite and zeros statistics.


```{r df_status, eval=FALSE}
## Profiling the data input
df_status(heart_disease)
```
<img src="dataset_profiling.png" width="500px" alt="Profiling data">

* `q_zeros`: quantity of zeros (`p_zeros`: in percentage)
* `q_inf`:  quantity of infinite values (`p_inf`: in percentage)
* `q_na`:  quantity of NA (`p_na`: in percentage)
* `type`: factor or numeric
* `unique`: quantity of unique values

### Why are these metrics important?

* **Zeros**: Variables with **lots of zeros** may be not useful for modeling, and in some cases it may dramatically bias the model.
* **NA**: Several models automatically exclude rows with NA (**random forest**, for example). As a result, the final model can be biased due to several missing rows because of only one variable. For example, if the data contains only one out of 100 variables with 90% of NAs, the model will be training with only 10% of original rows.
* **Inf**: Infinite values may lead to an unexpected behavior in some functions in R.
* **Type**: Some variables are encoded as numbers, but they are codes or categories, and the models **don't handle them** in the same way.
* **Unique**: Factor/categorical variables with a high number of different values (~30), tend to do overfitting if categories have low cardinality, (**decision trees**, for example).


<br>


### Filtering unwanted cases

The function `df_status` takes a data frame and returns an _status table_ that can help us to quickly remove features (or variables) based on all the metrics described in last section. For example:


**Removing variables with a _high number_ of zeros**

```{r profiling_data}
## Profiling the data input.
my_data_status=df_status(heart_disease, print_results = F)

# Removing variables with 60% of zero values
vars_to_remove=filter(my_data_status, p_zeros > 60)  %>% .$variable
vars_to_remove

## Keeping all columns except the ones present in 'vars_to_remove' vector
heart_disease_2=select(heart_disease, -one_of(vars_to_remove))
```


**Ordering data by percentage of zeros**

```{r profiling_data_2}
arrange(my_data_status, -p_zeros) %>% select(variable, q_zeros, p_zeros)
```

<br>

The same reasing applies when we want to remove -or keep- those variables above or below a certain threshold. Please check the missing values chapter to get more info about the implications when dealing with variables containing missing values.


### Going deep into these topics

Values return by `df_status` are deeply covered in other chapters:

* **Missing values** (NA) treatment, analysis and imputation are deeply cover in <a href="http://livebook.datascienceheroes.com/data_preparation/treating_missing_data.html">Missing Data</a> chapter.
* **Data types**, its conversions, implications when handling different data types and more, is covered in <a href="http://livebook.datascienceheroes.com/data_preparation/data_types.html">Data Types</a> chapter.
* A high number of **unique values** is a synonim of high cardinallity variable. This situation is studied in both chapters:
    + <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_descriptive_stats.html">High Cardinality Variable in Descriptive Stats</a>.
    + <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html">High Cardinality Variable in Predictive Modeling</a>.


<br>

## Profiling categorical variable

_Make sure you have the latest funModeling version (>= 1.3)._

Frequency or distribution analysis is made simple by the `freq` function. It retrieves the distribution in a table and a plot (by default) which shows the distribution in absolute and relative numbers.

If you want the distribution for two variables: 

```{r profiling_categorical_variable,fig.height=3, fig.width=5}
freq(data=heart_disease, str_input = c('thal','chest_pain'))
```

As well as in the remaining `funModeling` functions, if `str_input` is missing it will run for all factor or character variables present in given data frame:

```{r, eval=F}
freq(data=heart_disease)
```
<br>

Also, as the other plot functions in the package, if there is the need of exporting plots, add the `path_out` parameter (it will create the folder if it's not created yet)

```{r, eval=F}
freq(data=heart_disease, path_out='my_folder')
```

A more complete analysis in <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_descriptive_stats.html">High Cardinality Variable in Descriptive Stats</a>.

<br>

## Profiling numerical variable

In this type of variables, common metrics are the mean, max, min values

### About the data

It contains many indicators regarding world development. We

Data source: <a href="http://databank.worldbank.org/data/reports.aspx?source=2&Topic=11#" target="blank">http://databank.worldbank.org</a>. 

First we have to do some data wrangling. We are going to keep with the newest value per indicator.

```{r}
library(Hmisc)

# Loading data from book repository.
data_world=read.csv(file = "https://raw.githubusercontent.com/pablo14/data-science-live-book/master/exploratory_data_analysis/World_Development_Indicators.csv", header = T, stringsAsFactors = F, na.strings = "..")


# The magical function which keeps newest values for each metric. If you're not familiar with R, skip it...
max_ix<-function(d) 
{
  ix=which(!is.na(d))
  res=ifelse(length(ix)==0, NA, d[max(ix)])
  return(res)
}

data_world$newest_value=apply(data_world[,5:ncol(data_world)], 1, FUN=max_ix)

```

The columns `Series.Name` and `Series.Code` are the indicators to be analyzed. 
`Country.Name` and `Country.Code` are the countries. Each row represent a unique combination of country and indicator. 
Remaining columns, `X1990..YR1990.` (year 1990),`X2000..YR2000.` (year 2000), `X2007..YR2007.` (year 2007), and so on; indicates the metric value for that year, thus each column is a year. 

<br>

### Taking decision as data scientist

There are many `NA` because some countries don't have the measure of the indicator in those years. And here we have to **take a decision** as data scientist, that maybe neither will be the best nor the most accurate without asking to an expert -i.e. a sociologist- in these kind of data. 

What to do with the `NA` values? In these case, we are gonna to keep with the **newest value** for all the indicators. Perhaps this is not the best way to extract conclusions for a paper, we are going to compare some countries with information up to 2016 while others countries will be updated to 2009. This is a valid approach for the first anaylsis, to compare all the indicators with the newest data.

Other solution could have been to keep with the newest value, only if this number belongs to the last 5 years. It would reduce the number of countries to analyze.

### Profiling numerical data in R

We'll use the indicator: `SI.POV.DDAY`, which according to the documantion measures: _Poverty headcount ratio at $1.90 a day is the percentage of the population living on less than $1.90 a day at 2011 international prices._

One really usefull function to profile data is `describe` coming in `Hmisc` package

```{r}
## We keep with all the rows belonging to "SI.POV.DDAY" indicator
data_poverty=filter(data_world,  Series.Code=="SI.POV.DDAY")

library(Hmisc) # contains `describe` function

describe(data_poverty$newest_value)
```

Where: 
* `n`: quantity of non-`NA` rows. In this case it indicates there are `116` countries containing a number.
* `missing`: number of missing values. Summing this indicator to the `n` gives us total number of rows. Almost half of the countries have no data.
* `unique`: number of unique (or distinct) values.
* `Info`: An estimator of the amount of information present in the variable, not important at this point.
* `Mean`: The classical mean or average.
* Numbers: `.05`, `.10`, `.25`, `.50`, `.75`, `.90` and `.95 ` stands for the percentiles. These values are really useful since it help us to describe the distribution. Will be treated lately. I.e., `.05` is the 5th percentile.
* `lowest` and `highest`: The 5 lowest/highest values. Here can spot outliers, or errors in data. For example, the variable represents a percentage, thus it cannot contain negative values.

Getting other useful statistics: **total rows**, **total columns** and **column names**:

```{r}
# Total rows
nrow(data_poverty)

# Total columns
ncol(data_poverty)

# Column names
colnames(data_poverty)
```
 
<br>
 
### Using percentiles to get semantical descriptions

Percentile is such a crucial concept in data science, that we are going to have a deep cover in the book. It is used in profiling as well as evualating the performance of a predictive model.

#### How to calculate it?

Altough there are several methods to get the percentile, for example based on interpolations, the easist method is to order the numerical variable, and then to see _what is the maximum value if we want to choose the 50% of the population ordered from ascendently._

#### First example using the 50th percentile

The percentile is is based on two concepts, the numerical variable value which was previous ordered, and the population percentage. 
For example, in the data we're analyzing we got that percentile `.50` (which is also the `median`), was: 6 -_this 6 is a percentage in our data_. 

Thus we can state: _"Half of the countries have a maximum of 6% of poverty."_, or, _"Half of the countries have as much as 6% of poverty."_

#### Indicating where most of the values are

In descriptive statistics we want to descrbibe in general terms, the population. We can speak about ranges using two percentiles, let's take the percentiles 10th (`0.075`) and the 90th (`54.4`):

_The poverty ranges from 0.075% to 54.4% in 80% of the countries._ 80% because we did 90th-10th, focusing on the middle of the population.

If we consider the 80% as the majority of the population, then we could say: _"Normally, (or in general terms), the poverty goes from 0.07% to 54.4%"_. A semantical description.

We looked at the 80% of the population, which seems a good number to describe where most of the cases are. We could also have used the 90% range (pecentile 95th - 0.5th). 

#### Is it realed to quartile?

**Quartile:** is a formal name to the 25, 50, and 75th percentile (quarters or Q). If we look at the 50% of the population, we need to substract the 3rd quartile (or percentile 75th) to 1st quartile (percentile 25th), and we get where 50% of data in concentrated, also known as **inter quartile range**, or IQR.

#### Calculating quantiles 

If we want to get the 25th quantile, there is the `quantile`:

```{r, warning=FALSE}
## na.rm=T is necessary if we have null values, like we have in this case
p_25=quantile(data_poverty$newest_value, probs = 0.25, na.rm=T)
p_25

## Also we can get multiple quantiles at once:
p_quartile=quantile(data_poverty$newest_value, probs = c(0.25, 0.5, 0.75), na.rm=T)
p_quartile
```

#### Visuallzing percentiles

Plotting an histogram alongisde the places where each percentile is, can help to understand the concept:

```{r, profiling_numerical_variable, warning=FALSE, message=FALSE}
df_p=data.frame(value=p_quartile, quantile=c("25th", "50th", "75th"))

library(ggplot2)
ggplot(data_poverty, aes(newest_value)) + geom_histogram() +
  geom_vline(data=df_p, 
             aes(xintercept=value, 
                 colour = quantile),
             show.legend = TRUE, linetype="dashed") + theme_light()

```

Let's say if we sum all the grey bars before the 25th percentile, it will be as tall as the sum of all the grey bars after the 75th percentile. 

In last plot the IQR appears between the first and the last dashed line. Contains the 50% of the population.

#### Rank and top / bottom 'X'% concepts

Rank concept is the same as the one seen in competition, it allow us to answer _what is the country with the highest poverty rate?_
We'll use `dense_rank` function, from `ggplot2` package. It assigns the position (rank) to each country but we need theÃ­n reverse order, that is, assigning the `rank = 1` to the highest value.

```{r}
library(dplyr)

## Creating rank variable 
data_poverty$rank=dense_rank(-data_poverty$newest_value) 

# Ordering data by rank
data_poverty=arrange(data_poverty, rank)

# Printing first 5 results:
head(select(data_poverty, Country.Name, rank, newest_value))
```

We can also ask: _In which position is Uruguay?_

```{r}
filter(data_poverty, Country.Name=="Uruguay") %>% select(rank)
```



#### Top and bottom 'X'% concepts

Other questions that we maybe interested in aswered: _What is the value for which I get the top 10% of lowest values?_

Percentile 10th is the answer:

```{r}
quantile(data_poverty$newest_value, probs=.1, na.rm = T)
```

Working on the opposite: _What is the value for which I get the bottom 10% of highest values?_

Percentile 90th is the answer, `quantile(data_poverty$newest_value, probs=.1, na.rm = T)`. We can filter all the cases above this value



#### Using percentiles to spot and remove outliers

Percentiles can be used to spot outlier in univariate analysis, that is, considering the extreme values for each variable at a time.

Latelty we describe the general or "normal" population using the inter quartile range. If something is normal, then there must be cases that are _abnormal_. **When we define the normal, the abnormal concept emerges as its opposite.**

```{r}
## Also we can get multiple quantiles at once:
outlier_thresholds=quantile(data_poverty$newest_value, probs = c(0.01, 0.1, 0.9, 0.99), na.rm=T)
outlier_thresholds
```

Given the skewed shape of the variable distribution, in this case it _could_ makes sense to consider all values above the top 1% (value `newest_value > 76.15`). 

This is the approach shown in the <a href="http://livebook.datascienceheroes.com/data_preparation/outliers_treatment.html" target="blank">Outliers treatment</a> chapter.


### Comparing the median and mean

In skewed distributions, like the one 

<br>

### Plotting variable distribution

We are going to analyze `heart_disease` data.


```{r}
library(ggplot2)
library(reshape2) # contains 'melt' and 'cast' functions

# A gentle function to plot numerical data
plot_numerical <- function(data) 
{
  ## The concept of 'wide' and 'long' is crucial to understand how to pass the correct data to ggplot. The official documentation is quite clear about it: http://seananderson.ca/2013/10/19/reshape.html
  wide_data=melt(data)
  ggplot(data = melt(wide_data), mapping = aes(x = value)) + 
    geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x') +  aes(fill = variable) 
}

## Plotting all numerical variables
plot_numerical(heart_disease)

v=c("gender", "chest_pain", "fasting_blood_sugar", "resting_electro", "thal", "exter_angina", "has_heart_disease")
hd=select(heart_disease, -one_of(v))
plot_numerical(hd)

```

The messages:
* `Removed 4 rows containing non-finite values (stat_bin).` indicates the function excluded 4 rows contaning missing values.
* `Using gender, chest_pain, fasting_blood_sugar, resting_electro, thal, exter_angina, has_heart_disease, variable as id variables` is due to the `melt`. Not relevant by now, it doesn't affect the final result. More information at: http://seananderson.ca/2013/10/19/reshape.html.

Use `plot_numerical` function as you pleased with your own data.

<br>

### Profiling data quantitatively

```{r}
library(moments)

profiling_num <- function(data, digits=2, print_results=T)
{
	if(missing(print_results))
		print_results=T

	df_res=data.frame(
		avg=sapply(data, function(x) mean(x, na.rm=T)),
		std_dev=sapply(data, function(x) sd(x, na.rm=T)),
  	p_01=sapply(data, function(x) quantile(x, probs = 0.01, na.rm=T)),
	  p_05=sapply(data, function(x) quantile(x, probs = 0.05, na.rm=T)),
		p_10=sapply(data, function(x) quantile(x, probs = 0.10, na.rm=T)),
		p_25=sapply(data, function(x) quantile(x, probs = 0.25, na.rm=T)),
		p_50=sapply(data, function(x) quantile(x, probs = 0.50, na.rm=T)),
		p_75=sapply(data, function(x) quantile(x, probs = 0.75, na.rm=T)),
		p_90=sapply(data, function(x) quantile(x, probs = 0.90, na.rm=T)),
		p_95=sapply(data, function(x) quantile(x, probs = 0.95, na.rm=T)),
		p_99=sapply(data, function(x) quantile(x, probs = 0.99, na.rm=T)),
		
  	skewness=sapply(data, function(x) skewness(x, na.rm=T)),
		kurtosis=sapply(data, function(x) kurtosis(x, na.rm=T)),
		iqr=sapply(data, function(x) IQR(x, na.rm=T))
	)

	df_res$variation_coef=df_res$std_dev/df_res$avg
	df_res$range_98=sprintf("[%s, %s]", df_res$p_01, df_res$p_99)
	df_res$range_80=sprintf("[%s, %s]", df_res$p_10, df_res$p_90)
	  
	df_res=select(df_res, -p_10, -p_90)
	
	## Create new variable for column name
	df_res$variable=rownames(df_res)
	rownames(df_res)=NULL

	# setting digits
	#options(digits=digits)
  
	# reordering columns
	df_res=select(df_res, variable, everything(), skewness, kurtosis, iqr)

	## Print or return results
	if(df_res) print(digits) else return(df_res)
}


```







