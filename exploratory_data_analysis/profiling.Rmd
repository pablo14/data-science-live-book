# Profiling Data


## What is this about?

Quantity of zeros, NA, Inf, unique values; as well as the data type may lead to a good or bad model. Here's an approach to cover the very first step in data modeling. 

```{r ,results="hide", echo=FALSE}
library(knitr)
knitr::opts_chunk$set(out.width="400px", dpi=120)
knitr::opts_knit$set(base.dir = "exploratory_data_analysis")
```

```{r lib, results="hide", message=FALSE}
## Loading funModeling !
library(funModeling)
library(dplyr)
data(heart_disease)
```

## Checking NA, zeros, data type and unique values

```{r df_status}
my_data_status=df_status(heart_disease)
```

* `q_zeros`: quantity of zeros (`p_zeros`: in percentage)
* `q_inf`:  quantity of infinite values (`p_inf`: in percentage)
* `q_na`:  quantity of NA (`p_na`: in percentage)
* `type`: factor or numeric
* `unique`: quantity of unique values

### Why are these metrics important?

* **Zeros**: Variables with **lots of zeros** may be not useful for modeling, and in some cases it may dramatically bias the model.
* **NA**: Several models automatically exclude rows with NA (**random forest**, for example). As a result, the final model can be biased due to several missing rows because of only one variable. For example, if the data contains only one out of 100 variables with 90% of NAs, the model will be training with only 10% of original rows.
* **Inf**: Infinite values may lead to an unexpected behavior in some functions in R.
* **Type**: Some variables are encoded as numbers, but they are codes or categories, and the models **don't handle them** in the same way.
* **Unique**: Factor/categorical variables with a high number of different values (~30), tend to do overfitting if categories have low cardinality, (**decision trees**, for example).

<br>

### Filtering unwanted cases

The function `df_status` takes a data frame and returns a the status table to quickly remove unwanted cases.


**Removing variables with high number of NA/zeros**

```{r profiling_data}
# Removing variables with 60% of zero values
vars_to_remove=filter(my_data_status, p_zeros > 60)  %>% .$variable
vars_to_remove

## Keeping all columns except vars_to_remove
heart_disease_2=select(heart_disease, -one_of(vars_to_remove))
```

**Ordering data by percentage of zeros**

```{r profiling_data_2}
arrange(my_data_status, -p_zeros) %>% select(variable, q_zeros, p_zeros)
```

<br>


## Profiling categorical variable

_Make sure you have the latest funModeling version (>= 1.3)._

Frequency or distribution analysis is made simple by the `freq` function. It retrieves the distribution in a table and a plot (by default) which shows the distribution in absolute and relative numbers.

If you want the distribution for two variables: 

```{r profiling_categorical_variable,fig.height=3, fig.width=5}
freq(data=heart_disease, str_input = c('thal','chest_pain'))
```

As well as in the remaining `funModeling` functions, if `str_input` is missing it will run for all factor or character variables present in given data frame:

```{r, eval=F}
freq(data=heart_disease)
```
<br>

Also, as the other plot functions in the package, if there is the need of exporting plots, add the `path_out` parameter (it will create the folder if it's not created yet)

```{r, eval=F}
freq(data=heart_disease, path_out='my_folder')
```

