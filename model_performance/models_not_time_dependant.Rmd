# Methodological Aspects on Model Validation
## Not Time Dependant

### What is this about?

Once we've built a predictive model, how sure we are about its quality? Did it captured general patterns _-information-_ (excluding the _-noise-_)? 


```{r ,results="hide", echo=FALSE}
library(knitr)
opts_knit$set(base.dir = "model_performance")
```

<br>

#### What sort of data?

Unlike <a href='http://livebook.datascienceheroes.com/model_performance/models_time_dependant.html'>time dependant models</a> in this case you'll have an data's snapshot at certain point of time, no new information will be generated. 

For example some health data reasearch picked from a reduced amount of people, a survey, some data available on internet for practicing purposes. It's not possible to add new cases either it's expensive or impossible.

The `heart_disease` data coming in `funModeling` package is other example. 

<br>

### Reducing unexpected behavior

When a model is trained it just sees a part of a reality. It's a sample from a populuation that cannot be totally seen. 

There are lots of ways to validate a model (Accuracy / ROC curves / Lift / Gain / etc). Any of these metrics are attached to variance, which implies getting different values if you remove some cases and then fit a new model, you'll see an _slighty_ different value. 

Imagine we fit a model and achieve an accuracy of "`81`", now remove 10% of the cases, fit a new model, the accuracy now is: `78.4`. What is the real accuracy? The one obtained with 100% of data or the other one? For example if the model will run live on production, it will see other cases and the accuracy point will move to a new one. 

_So what is the real value? The one to report?_ Resampling and cross-validation techniques will average -base on different sampling and testing criteria- in order to retrieve an approximation to the most trusted value.

<br> 

**But why removing cases?**

There is no sense in removing cases like that, but it gets an idea about how sensible the accuracy metric is, remember you're working with a sample from an *_unknown populuation_*. 

> If we'd have a fully deterministic model, a model that contains 100% of all cases we are studying, and predictions were 100% accurated in all cases, we wouldn't need all of this. 

> As far as we always analyze samples, we just need to getting closer to the _real truthness_ of data thorugh repetition, re-sampling, cross-validation, and so on...

<br>

### Let's illustrate this with Cross-Validation (CV)

<img src="k-fold_cross_validation.png" width="400px" alt="Cross-Validation">

Source: [1]

<br>

#### CV short summary

* Splits the data into random groups, let's say `10`, equally sized. These groups are commonly called `folds`, represented by `'k'` letter.
* Take `9` folds, build a model, and then apply the model to the remaining fold (the one which was lef-out). This will return the accuracy metric you want: accuracy, ROC, Kappa, etc. Use accuracy in this example.
* Repeat this `k` times (`10` in our example). So we'll get `10` different accuracies. Final result will be the average of all of them. 

This average will be the one to take the decision if a model is good or not, and also to include it in a report. 

<br>

#### Practical example

There 150 rows in `iris` data frame, using <a href="http://topepo.github.io/caret/index.html">caret package</a> so building a `random forest` with `caret` using `cross-validation` will end up in the -internal- construction of 10 random forest, each one based on 135 rows (9/10 * 150), and reporting an accuracy based on remaining 15 (1/10 * 150) cases. This procedure 10 times.

This part of the output: 

<img src="caret_cross_validation_output.png">

`Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... `, each 135 represents a training sample, 10 in total but the output is shrinked.

Rather a single number -the average-, we can see a distribution:

<img src="accuracy_distribution_plot.png" width="200px">

<img src="accuracy_distribution.png" width="200px">

* The min/max accuracy will be between `~0.8` and `~1`.
* The mean is the one reported by `caret`.
* 50% of times it will be ranged between `from ~0.93 to ~1`.

<br>

#### What to use in practice?

It depends on the data, but it's common to find examples cases `10 fold CV`, plus repetition: `10 fold CV, repeated 5 times`.  In `caret` it is called `repeatedcv`. And using the average of the desiered metric. Using the `ROC` for being less biased to unbalanced target variable. Check [scoring chapter].

Since these validation techniques are **time consuming**, consider choosing a model which will run fast, allowing model tunning, test different confirgurations, try different variables in a "short" amount of time. **<a href="https://www.stat.berkeley.edu/~breiman/RandomForests/" target="blank">**Random Forest</a>** are an excelent option given **fast** and **accurated** results [2]. 

**A parenthesis in random forest...**

This model uses a resampling technique (bootrsapping), to build `N` trees (normally hundreds/thousands), where each one had seen sample of the data. Final decision is the average of all trees which were built having seen **different information**, plus, each one used a **different a set of variables** so they try to be not correlated between them:

In general terms each tree had been built using: 

* **Different input data** (samples)
* **Different input variables.** 

<br>

### Don't forget: Data Preparation

Tweeking input data by transforming and cleaning it, will impact on model quality. Sometimes more than optimizing the model through its parameters. 
This book has a <a href="http://livebook.datascienceheroes.com/data_preparation/introduction.html">Data Preparation</a> section in early stage. 


### Final toughts

* Validating the models through re-sampling / cross-validation help us to estimate the "real" error in present in data. If the model will run in a future, it will be the expected error to have.
* Another advantage is **model tunning**, avoiding the overfitting in selecting best parameters for certain model, <a href="https://topepo.github.io/caret/model-training-and-tuning.html" target="blank">Example in `caret`</a> 
* To have many opinions / samples / folds tend to produce 

> The best test is the one made by you, suited to your data and needs. 

Try different models and analyze the tradeoff between time consumption and any accuracy metric.

<br>

#### Further reading
 
* Tutorial: <a href="http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r">Cross validation for predictive analytics using r</a>
* Tutorial by Max Kahn (caret's creator): <a href="http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm">Comparing Different Species of Cross-Validation</a>  
* [1] Image source: <a href="http://sebastianraschka.com/faq/docs/evaluate-a-model.html" target="blank">Image source</a>
* [2] More on Random Forest overall performance: <a href="http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf">Do we Need Hundreds of Classiers to Solve Real World Classication Problems?</a>
* Rob Hyndman (the creator of `forecast` package) <a href="http://robjhyndman.com/hyndsight/crossvalidation/">Why every statistician should know about cross-validation?</a> 







