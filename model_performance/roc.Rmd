ROC and KS Metrics 
====

_Before continue, it's highly recommended to see the <a href="http://livebook.datascienceheroes.com/scoring/scoring.html">scoring chapter.</a>_

### What is this about?

Once the predictive model is developed with `training` data, it should be compared with `test` data (which wasn't seen by the model before). Here is presented a wrapper for the **ROC Curve** and **AUC** (area under ROC) and  the **KS** (Kolmogorov-Smirnov).

```{r ,results="hide", echo=FALSE}
library(knitr)
opts_knit$set(base.dir = "model_performance")
```

```{r lib, results="hide"}
## Loading funModeling !
suppressMessages(library(funModeling))
data(heart_disease)
```

### Creating the model
```{r model_perfomance1}
## Training and test data. Percentage of training cases default value=80%.
index_sample=get_sample(data=heart_disease, percentage_tr_rows=0.8)

## Generating the samples
data_tr=heart_disease[index_sample,] 
data_ts=heart_disease[-index_sample,]


## Creating the model only with training data
fit_glm=glm(has_heart_disease ~ age + oldpeak, data=data_tr, family = binomial)

```

### ROC, AUC and KS performance metrics
```{r model_perfomance2,  fig.height=3, fig.width=4}
## Performance metrics for Training Data
model_performance(fit=fit_glm, data = data_tr, target_var = "has_heart_disease")

## Performance metrics for Test Data
model_performance(fit=fit_glm, data = data_ts, target_var = "has_heart_disease")
```

**Key notes**

* The **higher** the KS and AUC, the **better** the performance is.
    + KS range: from 0 to 1.
    + AUC range: from 0.5 to 1.
* Performance metrics should be **similar** between training and test set.

**Final comments**

* KS and AUC focus on similar aspects: How well the model distinguishes the class to predict.
* ROC and AUC article: https://en.wikipedia.org/wiki/Receiver_operating_characteristic


